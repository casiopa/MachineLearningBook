{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensambles de árboles de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting  \n",
    "\n",
    "- Generaliza el concepto de boosting a cualquier función de costo derivable\n",
    "- Cada clasificador en la cadena se entrena con los residuos del clasificador anterior\n",
    "\n",
    "En scikit-learn en el módulo [`ensemble`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble) se encuentra Gradient Boosting para clasificar y hacer regresión\n",
    "\n",
    "    sklearn.ensemble.GradientBoostingClassifier(loss=’deviance’, learning_rate=0.1, \n",
    "                                                n_estimators=100, subsample=1.0, \n",
    "                                                max_depth=3, ...)\n",
    "                                                \n",
    "Esta implementación usa árboles como clasificador débil\n",
    "\n",
    "Explicación de los parámetros (algunos):\n",
    "- n_estimators: Número de árboles\n",
    "- max_depth: Profundidad de los árboles\n",
    "- subsample: Se usa para que cada árbol use una submuestra del dataset\n",
    "- learning_rate: Se usa para disminuir la contribución de cada árbol sucesivo\n",
    "- max_features: Número de atributos a considerar en cada split (reduce la varianza)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontremos el mejor ensamble usando 5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "params = {'loss':('deviance', 'exponential'), \n",
    "          'max_depth':[1, 5, 10, 20],\n",
    "          'n_estimators': [1, 10, 20, 50, 100]}\n",
    "model = ensemble.GradientBoostingClassifier(subsample=0.5, learning_rate=0.1, max_features=None)\n",
    "gbs = GridSearchCV(model, params, cv=5)\n",
    "gbs.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = gbs.cv_results_['mean_test_score'][gbs.cv_results_['param_loss']== 'deviance']\n",
    "stds = gbs.cv_results_['std_test_score'][gbs.cv_results_['param_loss']== 'deviance']\n",
    "for mean, std, params in zip(means, stds, gbs.cv_results_['params']):\n",
    "    print(\"score: %0.3f (+/-%0.03f) con %r\"\n",
    "          % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mejor modelo es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gbs.best_estimator_\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting funciona bien con árboles poco profundo\n",
    "\n",
    "Clasificador débil con alto sesgo y baja varianza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3), tight_layout=True)\n",
    "\n",
    "ax[0].contourf(xx, yy, Z.reshape(xx.shape), cmap=plt.cm.RdBu, alpha=0.5)\n",
    "ax[0].scatter(X[Y==0, 0], X[Y==0, 1], color='k', s=10, marker='o', alpha=0.5)\n",
    "ax[0].scatter(X[Y==1, 0], X[Y==1, 1], color='k', s=10, marker='x', alpha=0.5)\n",
    "fpr, tpr, th = roc_curve(Y_train, model.predict_proba(X_train)[:, 1])\n",
    "ax[1].plot(fpr, tpr, label='Entrenamiento', linewidth=2)\n",
    "fpr, tpr, th = roc_curve(Y_test, model.predict_proba(X_test)[:, 1])\n",
    "ax[1].plot(fpr, tpr, label='Prueba', linewidth=2)\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "ax[1].set_ylim([0.0, 1.0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos comparar el rendimiento del mejor árbol con el mejor ensamble en el conjunto de Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(5, 4), tight_layout=True)\n",
    "ax.set_xlabel('FPR')\n",
    "ax.set_ylabel('TPR/Recall')\n",
    "\n",
    "Y_pred = dts.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, th = roc_curve(Y_test, Y_pred)\n",
    "ax.plot(fpr, tpr, label=\"Decision Tree %0.4f\" %auc(fpr, tpr), linewidth=2)\n",
    "\n",
    "Y_pred = gbs.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, th = roc_curve(Y_test, Y_pred)\n",
    "ax.plot(fpr, tpr, label=\"Gradient boosting %0.4f\" %auc(fpr, tpr), linewidth=2)\n",
    "plt.legend(loc=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "- Conjunto de árboles de decisión entrenados en paralelo usando bootstrap \n",
    "- Cada árbol se entrena con un **subconjunto aleatorio** de los datos (bagging)\n",
    "- Cada árbol se entrena con un **subconjunto aleatorio** de los atributos (random forest)\n",
    "\n",
    "Nuevamente en el módulo [`ensemble`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble) podemos encontrar random forest para clasificar y hacer regresión\n",
    "\n",
    "    sklearn.ensemble.RandomForestClassifier(n_estimators=’warn’, criterion=’gini’, \n",
    "                                            max_depth=None,\n",
    "                                            max_features=’auto’, bootstrap=True, \n",
    "                                            oob_score=False,\n",
    "                                            n_jobs=None, class_weight=None, ...)\n",
    "                                                \n",
    "\n",
    "Explicación de los parámetros (algunos):\n",
    "- n_estimators: Número de árboles\n",
    "- max_depth: Profundidad de los árboles\n",
    "- max_features: Número de atributos a considerar en cada split\n",
    "- criterion: Para decidir como se escogen los splits\n",
    "- bootstrap: Muestreo con reemplazo (desactiva bagging)\n",
    "- class_weight: Usar o no una mayor ponderación para las clases menos representadas\n",
    "- n_jobs: número de cores para entrenar\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'criterion':('entropy', 'gini'),\n",
    "          'max_depth':[1, 5, 10, 20],\n",
    "          'n_estimators': [1, 10, 20, 50, 100]}\n",
    "model = ensemble.RandomForestClassifier(max_features=None, n_jobs=-1)\n",
    "rfs = GridSearchCV(model, params, cv=5)\n",
    "rfs.fit(X_train, Y_train)\n",
    "display(rfs.best_estimator_)\n",
    "display(rfs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mejor Random Forest es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rfs.best_estimator_\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A diferencia de GB, Random Forest prefiere árboles más profundos\n",
    "\n",
    "Clasificadores débiles con bajo sesgo y alta varianza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3), tight_layout=True)\n",
    "\n",
    "ax[0].contourf(xx, yy, Z.reshape(xx.shape), cmap=plt.cm.RdBu, alpha=0.5)\n",
    "ax[0].scatter(X[Y==0, 0], X[Y==0, 1], color='k', s=10, marker='o', alpha=0.5)\n",
    "ax[0].scatter(X[Y==1, 0], X[Y==1, 1], color='k', s=10, marker='x', alpha=0.5)\n",
    "fpr, tpr, th = roc_curve(Y_train, model.predict_proba(X_train)[:, 1])\n",
    "ax[1].plot(fpr, tpr, label='Entrenamiento', linewidth=2)\n",
    "fpr, tpr, th = roc_curve(Y_test, model.predict_proba(X_test)[:, 1])\n",
    "ax[1].plot(fpr, tpr, label='Prueba', linewidth=2)\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "ax[1].set_ylim([0.0, 1.0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos comparar el rendimiento del mejor árbol con los mejores ensambles Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(5, 4), tight_layout=True)\n",
    "ax.set_xlabel('FPR')\n",
    "ax.set_ylabel('TPR/Recall')\n",
    "\n",
    "Y_pred = dts.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, th = roc_curve(Y_test, Y_pred)\n",
    "ax.plot(fpr, tpr, label=\"Decision Tree %0.4f\" %auc(fpr, tpr), linewidth=2)\n",
    "\n",
    "Y_pred = gbs.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, th = roc_curve(Y_test, Y_pred)\n",
    "ax.plot(fpr, tpr, label=\"Gradient boosting %0.4f\" %auc(fpr, tpr), linewidth=2)\n",
    "\n",
    "Y_pred = rfs.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, th = roc_curve(Y_test, Y_pred)\n",
    "ax.plot(fpr, tpr, label=\"Random Forest %0.4f\" %auc(fpr, tpr), linewidth=2)\n",
    "plt.legend(loc=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Más sobre ensambles\n",
    "\n",
    "Dos algoritmos de Gradient Boosting para árboles de decisión (GBDT) extremadamente competitivos:\n",
    "\n",
    "- [XGBoost](http://dmlc.cs.washington.edu/xgboost.html)\n",
    "- [LightGBM](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree) e [implementación oficial](https://lightgbm.readthedocs.io/en/latest/index.html)\n",
    "\n",
    "Ambos implementan estrategias para mejorar la eficiencia y realizar cálculos paralelos/distribuidos e incluso usando GPU\n",
    "\n",
    "Estado del arte en clasificación de datos estructurados (tablas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
